{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Ch Mubashir_56892.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Dummy Environment and Data\n",
        "# -----------------------------\n",
        "# Assume we have a simple environment with 4 states and 2 actions\n",
        "num_states = 4\n",
        "num_actions = 2\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "# Random example episode\n",
        "episode_states = torch.tensor([[0,1,0,0],\n",
        "                               [1,0,0,0],\n",
        "                               [0,0,1,0]], dtype=torch.float32)  # 3 steps\n",
        "episode_actions = torch.tensor([0,1,0])\n",
        "episode_rewards = torch.tensor([1.0, 0.5, 2.0])\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Define a simple Policy Network\n",
        "# -----------------------------\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.softmax(self.fc(x), dim=-1)\n",
        "\n",
        "policy_net = PolicyNetwork(num_states, num_actions)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Calculate Cumulative Reward\n",
        "# -----------------------------\n",
        "cumulative_reward = 0\n",
        "discounted_rewards = []\n",
        "for r in reversed(episode_rewards):\n",
        "    cumulative_reward = r + gamma * cumulative_reward\n",
        "    discounted_rewards.insert(0, cumulative_reward)\n",
        "discounted_rewards = torch.tensor(discounted_rewards)\n",
        "print(\"Discounted / Cumulative Rewards:\", discounted_rewards)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Compute Policy Loss (REINFORCE)\n",
        "# -----------------------------\n",
        "log_probs = torch.log(policy_net(episode_states).gather(1, episode_actions.unsqueeze(1)).squeeze())\n",
        "policy_loss = -torch.sum(log_probs * discounted_rewards)\n",
        "print(\"Policy Loss:\", policy_loss.item())\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Compute Value Function & Bellman Error\n",
        "# -----------------------------\n",
        "# Dummy value network\n",
        "value_net = nn.Linear(num_states, 1)\n",
        "predicted_values = value_net(episode_states).squeeze()\n",
        "\n",
        "# Bellman target: R_t + gamma * V(s_{t+1})\n",
        "next_values = torch.cat([predicted_values[1:], torch.tensor([0.0])])\n",
        "bellman_target = episode_rewards + gamma * next_values\n",
        "bellman_error = nn.MSELoss()(predicted_values, bellman_target)\n",
        "print(\"Bellman Error (MSE between V and target):\", bellman_error.item())\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Compute Mean Squared Error (MSE) Example\n",
        "# -----------------------------\n",
        "# Just compute MSE between predicted values and discounted rewards\n",
        "mse_loss = nn.MSELoss()(predicted_values, discounted_rewards)\n",
        "print(\"Mean Squared Error (MSE):\", mse_loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_-degZmQVFu",
        "outputId": "3708ba0c-d700-4036-9ae7-7584cb966619"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discounted / Cumulative Rewards: tensor([3.4552, 2.4800, 2.0000])\n",
            "Policy Loss: 4.843930244445801\n",
            "Bellman Error (MSE between V and target): 1.569450855255127\n",
            "Mean Squared Error (MSE): 7.567729949951172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Metrics:\n",
        "\n",
        "Cumulative Reward / Discounted Reward :\n",
        "\n",
        "Sum of rewards with discount factor applied.\n",
        "\n",
        "Measures total “goodness” of an episode.\n",
        "\n",
        "Policy Loss :\n",
        "\n",
        "For REINFORCE, policy loss = -sum(log_prob * discounted_reward)\n",
        "\n",
        "Maximizing cumulative reward by updating policy parameters.\n",
        "\n",
        "Bellman Error :\n",
        "\n",
        "Difference between predicted value V(s) and Bellman target R + gamma*V(s').\n",
        "\n",
        "Used in value-based or actor-critic methods.\n",
        "\n",
        "Mean Squared Error (MSE) :\n",
        "\n",
        "MSE between predicted value and discounted reward (alternative baseline). **bold text**"
      ],
      "metadata": {
        "id": "fPuRc3GsOTG2"
      }
    }
  ]
}